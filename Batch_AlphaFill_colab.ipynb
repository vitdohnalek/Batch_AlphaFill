{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "owU0YqxM6Vsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Input & Output Info\n",
        "import os\n",
        "\n",
        "Input_folder = '/content/gdrive/My Drive/' #@param {type:\"string\"}\n",
        "#@markdown  - Folder containing files in pdb/cif format\n",
        "Output_folder = '/content/gdrive/My Drive/' #@param {type:\"string\"}\n",
        "Results_file = '/content/gdrive/My Drive/AlphaFill_results.tsv' #@param {type:\"string\"}\n",
        "Add_ligand_names = True #@param {type:\"boolean\"}\n",
        "#@markdown  - Append ligand names to the ligand IDs\n",
        "\n",
        "if Add_ligand_names == True:\n",
        "  os.system(\"wget https://raw.githubusercontent.com/vitdohnalek/Batch_AlphaFill/main/ligands_info.json\")\n",
        "\n",
        "  #Adds ligands names to the AlphaFill results table\n",
        "  def add_ligand_names(ligand_info_file, table, output_suffix=\"ligand_names\"):\n",
        "\n",
        "      table_name = os.path.splitext(os.path.basename(table))[0]\n",
        "      new_table = \"\"\n",
        "\n",
        "      with open(ligand_info_file, \"r\") as json_file:\n",
        "          ligand_info = json.load(json_file)\n",
        "\n",
        "      with open(table, \"r\") as f:\n",
        "          for l in f:\n",
        "              line = l.strip().split(\"\\t\")\n",
        "              new_table += line[0] + \"\\t\"\n",
        "              for ligand in line[1:]:\n",
        "                  if len(ligand.split()) == 1:\n",
        "                      new_table += ligand + \"; \" + ligand_info[ligand].replace(\" \",\"_\") + \"\\t\"\n",
        "                  else:\n",
        "                      new_table += ligand + \"; \" + ligand_info[ligand.split()[0]].replace(\" \",\"_\") + \"\\t\"\n",
        "              new_table = new_table.strip() + \"\\n\"\n",
        "\n",
        "      if output_suffix != \"\":\n",
        "        with open(f\"{table_name}_{output_suffix}.tsv\", \"w\") as f:\n",
        "            f.write(new_table)\n",
        "      else:\n",
        "        with open(f\"{table_name}.tsv\", \"w\") as f:\n",
        "            f.write(new_table)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6Hw9XK8n7EvQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z2p7taib46_0"
      },
      "outputs": [],
      "source": [
        "#@title AlphaFill\n",
        "#Script for batch AlphaFill predictions using the https://alphafill.eu/ API\n",
        "#Input is a folder containing pdb/cif files\n",
        "#Structure files are enriched by hypothetical ligands\n",
        "#List of possible ligand binding sites is stored as .tsv file\n",
        "#JSON file is produced for each structure with all details\n",
        "\n",
        "import requests\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "#Finds values in JSON file\n",
        "def find_values(json_obj, key):\n",
        "    values = []\n",
        "\n",
        "    # Check if the current object is a dictionary\n",
        "    if isinstance(json_obj, dict):\n",
        "        for k, v in json_obj.items():\n",
        "            # If the key matches, add the corresponding value to the list\n",
        "            if k == key:\n",
        "                values.append(v)\n",
        "            # If the value is another dictionary, recursively search it\n",
        "            elif isinstance(v, dict):\n",
        "                nested_values = find_values(v, key)\n",
        "                if nested_values is not None:\n",
        "                    values.extend(nested_values)\n",
        "            # If the value is a list, iterate through the list and recursively search each element\n",
        "            elif isinstance(v, list):\n",
        "                for item in v:\n",
        "                    nested_values = find_values(item, key)\n",
        "                    if nested_values is not None:\n",
        "                        values.extend(nested_values)\n",
        "    # If the current object is a list, iterate through the list and recursively search each element\n",
        "    elif isinstance(json_obj, list):\n",
        "        for item in json_obj:\n",
        "            nested_values = find_values(item, key)\n",
        "            if nested_values is not None:\n",
        "                values.extend(nested_values)\n",
        "\n",
        "    return values if values else None  # Return an empty list if no values found\n",
        "\n",
        "#Retrives the ligands IDs\n",
        "def read_JSON(json_file):\n",
        "\n",
        "    #Reads the json data\n",
        "    with open(json_file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    ligands = find_values(data, \"analogue_id\")\n",
        "\n",
        "    return ligands\n",
        "\n",
        "#Connects to the AlphaFill server\n",
        "#Uploads the structure\n",
        "#Downloads the CIF results file & JSON file containing all details\n",
        "def AlphaFill(structure_file):\n",
        "\n",
        "    #Reads the file name\n",
        "    file_name = os.path.splitext(os.path.basename(structure_file))[0]\n",
        "\n",
        "    # Endpoint URL\n",
        "    url = 'https://alphafill.eu/v1/aff'\n",
        "\n",
        "    # Read the content of the input structure file\n",
        "    with open(structure_file, 'r') as file:\n",
        "        structure_file_content = file.read()\n",
        "\n",
        "    # Parameters for the POST request\n",
        "    payload = {\n",
        "        'structure': structure_file_content,\n",
        "    }\n",
        "\n",
        "    #Conditions for while loops\n",
        "    running_request = True\n",
        "    status_queued = True\n",
        "\n",
        "    while running_request:\n",
        "        # Makes the POST request\n",
        "        response = requests.post(url, data=payload)\n",
        "\n",
        "        # Continues when the response is 200\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            print(f'Success! Job ID: {result[\"id\"]}')\n",
        "\n",
        "            time.sleep(5) #Not sure if this is needed, it should give time to the server to handle the request\n",
        "\n",
        "            #Handles the status of the request\n",
        "            while status_queued:\n",
        "\n",
        "                #Gets the current status\n",
        "                status_url = f'https://alphafill.eu/v1/aff/{result[\"id\"]}/status'\n",
        "                status = requests.get(status_url).json()\n",
        "\n",
        "\n",
        "                #In case of error, the prediction for the current model is skipped\n",
        "                if status['status'] == 'error':\n",
        "                    print(f'Status: {status[\"status\"]}')\n",
        "                    status_queued = False\n",
        "                #Finished status enables the code to continue\n",
        "                elif status['status'] == 'finished':\n",
        "                    #print(status)\n",
        "                    status_queued = False\n",
        "                #Queued status makes the code wait 30 seconds before another status check\n",
        "                elif status['status'] == 'queued':\n",
        "                    print(f\"{result['id']} queued\")\n",
        "                    time.sleep(30)\n",
        "                #Running process makes the code waits 30 seconds before another check\n",
        "                elif status['status'] == 'running':\n",
        "                    print(f\"{result['id']} running; progress: {status['progress']}\")\n",
        "                    time.sleep(30)\n",
        "                #In case of anything unexpected the code waits 10 second\n",
        "                #This might not be needed\n",
        "                else:\n",
        "                    print(status)\n",
        "                    time.sleep(10)\n",
        "\n",
        "            #Retrives the structure in CIF format\n",
        "            structure_url = f\"https://alphafill.eu/v1/aff/{result['id']}\"\n",
        "            structure = requests.get(structure_url)\n",
        "            with open(f\"{file_name}_AlphaFill.cif\", \"wb\") as cif_file:\n",
        "                cif_file.write(structure.content)\n",
        "\n",
        "            #retrieves the additional informayion\n",
        "            json_url = f\"https://alphafill.eu/v1/aff/{result['id']}/json\"\n",
        "            json = requests.get(json_url)\n",
        "            with open(f\"{file_name}_AlphaFill.json\", \"wb\") as json_file:\n",
        "                json_file.write(json.content)\n",
        "\n",
        "        #Handles busy server error by checking the server every 10 seconds\n",
        "        #Stops the predictions in case of other errors\n",
        "        else:\n",
        "            error = response.json()\n",
        "            if error[\"error\"] == \"The server is too busy to handle your request, please try again later\":\n",
        "                print(f'Error: {error[\"error\"]}')\n",
        "                time.sleep(10)\n",
        "            else:\n",
        "                print(f'Error: {error[\"error\"]}')\n",
        "                running_request = False\n",
        "\n",
        "        #Everything is done!\n",
        "        running_request = False\n",
        "    print(f\"Prediction for {file_name} has been processed\")\n",
        "\n",
        "#Runs the predictions with all files in a given folder\n",
        "def batch_AlphaFill(folder, output_folder=\".\"):\n",
        "\n",
        "    folder = folder.strip('/')\n",
        "    for structure_file in glob.glob(f\"/{folder}/*\"):\n",
        "        AlphaFill(structure_file)\n",
        "\n",
        "        #Moves the files into a specific folder if requested\n",
        "        if output_folder != \".\":\n",
        "            output_folder = output_folder.strip('/')\n",
        "            #Reads the file name\n",
        "            file_name = os.path.splitext(os.path.basename(structure_file))[0]\n",
        "            os.system(f\"mv {file_name}_AlphaFill.* /{output_folder}/\")\n",
        "\n",
        "#Creates a table with all the ligand IDs\n",
        "def batch_JSON_results(folder, results_name=\"\"):\n",
        "\n",
        "    table_results = \"\"\n",
        "\n",
        "    #Iterates over JSON files and appends results to the table\n",
        "    folder = folder.strip('/')\n",
        "    for file in glob.glob(f\"{folder}/*_AlphaFill.json\"):\n",
        "\n",
        "        file_name = os.path.splitext(os.path.basename(file))[0]\n",
        "        ligands_info = \"\"\n",
        "\n",
        "        ligands = sorted(read_JSON(file))\n",
        "        unique_ligands = set(ligands)\n",
        "        #Counts the number of all ligands\n",
        "        #Sometimes that are multiple ligands of the same kind\n",
        "        for unique_ligand in unique_ligands:\n",
        "            unique_ligand_count = ligands.count(unique_ligand)\n",
        "            if unique_ligand_count == 1:\n",
        "                ligands_info += unique_ligand + \"\\t\"\n",
        "            else:\n",
        "                ligands_info += unique_ligand + \" \" + str(unique_ligand_count) + \"\\t\"\n",
        "\n",
        "        #Appends next row to the results\n",
        "        table_results += file_name + \"\\t\" + ligands_info.strip(\"\\t\") + \"\\n\"\n",
        "\n",
        "    with open(results_name, \"w\") as f:\n",
        "        f.write(table_results)\n",
        "\n",
        "    print(f\"Ligands have been stored in a table {results_name}\")\n",
        "\n",
        "#Runs the functions\n",
        "batch_AlphaFill(Input_folder)\n",
        "for file in glob.glob(\"*.json\"):\n",
        "  shutil.copy(file, Output_folder)\n",
        "for file in glob.glob(\"*.cif\"):\n",
        "  shutil.copy(file, Output_folder)\n",
        "\n",
        "time.sleep(10)\n",
        "\n",
        "batch_JSON_results(\".\", results_name=Results_file.split('/')[-1])\n",
        "shutil.copy(Results_file.split('/')[-1], Results_file)\n",
        "\n",
        "if Add_ligand_names == True:\n",
        "  add_ligand_names(ligand_info_file=\"ligands_info.json\", table=Results_file.split('/')[-1], output_suffix=\"\")\n",
        "  shutil.copy(Results_file.split('/')[-1], Results_file)"
      ]
    }
  ]
}